# -*- coding: utf-8 -*-
"""2-2.(실습)image_DNN모델_fashion_mnist

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qy3RABNe70rCzCrC0JbRwbc5lp1fljn_

# Category 2

Fully Connected Layer (Dense)를 활용한 이미지 분류 (Image Classification)

## 확인

1. GPU 옵션 켜져 있는지 확인할 것!!! (수정 - 노트설정 - 하드웨어설정 (GPU))

## 순서

1. **import**: 필요한 모듈 import
2. **전처리**: 학습에 필요한 데이터 전처리를 수행합니다.
3. **모델링(model)**: 모델을 정의합니다.
4. **컴파일(compile)**: 모델을 생성합니다.
5. **학습 (fit)**: 모델을 학습시킵니다.

## 문제

Create a classifier for the Fashion MNIST dataset

Note that the test will expect it to classify 10 classes and that 

the input shape should be the native size of the Fashion MNIST dataset which is 28x28 monochrome. 

Do not resize the data. Your input layer should accept

(28,28) as the input shape only. 

If you amend this, the tests will fail.

Fashion MNIST 데이터 셋에 대한 분류기 생성
테스트는 10 개의 클래스를 분류 할 것으로 예상하고

입력 모양은 Fashion MNIST 데이터 세트의 기본 크기 여야합니다.28x28 단색. 

데이터 크기를 조정하지 마십시오. input_shape는
(28,28)을 입력 모양으로 만 사용합니다.

## import 하기

필요한 모듈을 import 합니다.
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint

"""## Load dataset

**tf.keras.datasets** 에는 다양한 샘플 데이터를 제공해줍니다.

[tf.keras.datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)

* **boston_housing**: Boston housing price regression dataset.

* **cifar10**: CIFAR10 small images classification dataset.

* **cifar100**: CIFAR100 small images classification dataset.

* **fashion_mnist**: Fashion-MNIST dataset.

* **imdb**: IMDB sentiment classification dataset.

* **mnist**: MNIST handwritten digits dataset.

* **reuters**: Reuters topic classification dataset.
"""

fashion_mnist = tf.keras.datasets.fashion_mnist

"""아래 순서가 헷갈릴 수 있습니다. 순서에 유의해 주세요."""

# 반드시 load_data() !!
(x_train, y_train), (x_valid, y_valid) = fashion_mnist.load_data()

x_train.shape, x_valid.shape  # (데이터 크기, 이미지 가로, 이미지 세로)

y_train.shape, y_valid.shape

"""**이미지 정규화 (Normalization)**

* 모든 이미지 픽셀(pixel)값들을 0~1 사이의 값으로 정규화 해 줍니다.
* x_train, x_valid 에 대해서만 정규화합니다.

정규화(Normalization) 전의 최소값(min), 최대값(max)을 확인합니다.
"""

x_train.min(), x_train.max()

"""정규화(Normalization) 합니다."""

x_train = x_train / 255.0
x_valid = x_valid / 255.0

"""정규화 후 최소값/최대값 확인"""

x_train.min(), x_train.max()

"""## 샘플 데이터 Visualization"""

# 시각화. 시험때는 굳이 필요없음 
fig, axes = plt.subplots(2, 5)
fig.set_size_inches(10, 5)

for i in range(10):
    axes[i//5, i%5].imshow(x_train[i], cmap='gray')
    axes[i//5, i%5].set_title(str(y_train[i]), fontsize=15)
    plt.setp( axes[i//5, i%5].get_xticklabels(), visible=False)
    plt.setp( axes[i//5, i%5].get_yticklabels(), visible=False)
    axes[i//5, i%5].axis('off')

plt.tight_layout()
plt.show()

"""* 0: 티셔츠/탑
* 1: 바지
* 2: 풀오버(스웨터의 일종)
* 3: 드레스
* 4: 코트
* 5: 샌들
* 6: 셔츠
* 7: 스니커즈
* 8: 가방
* 9: 앵클 부츠

## Flatten Layer

**Flatten이란?**

* 고차원을 1D로 변환하여 **Dense Layer에 전달**해 주기 위하여 사용합니다.
* 28 X 28 의 **2D**로 되어 있는 이미지를 784로 **1D로 펼쳐 주는 작업**입니다.

**변환 전**
"""

x_train.shape

"""**Flatten 적용 후**"""

x = Flatten(input_shape=(28, 28))   # 28*28
print(x(x_train).shape)

"""## 모델 정의 (Sequential)

이제 Modeling을 할 차례입니다.

`Sequential` 모델 안에서 층을 깊게 쌓아 올려 주면 됩니다.

1. Dense 레이어는 2D 형태의 데이터를 받아들이지 못합니다. Flatten 레이어로 2D -> 1D로 변환해주세요
2. 깊은 출력층과 더 많은 Layer를 쌓습니다.
3. Dense Layer에 `activation='relu'`를 적용합니다.
4. 분류(Classification)의 마지막 층의 출력 숫자는 분류하고자 하는 클래스 갯수와 **같아야** 합니다.
"""

x_train.shape

model = Sequential([
    # Flatten으로 shape 펼치기
    Flatten(input_shape=(28, 28)), # x_train의 shape! 
    # Dense Layer
    Dense(1024, activation='relu'),  # 각자 Dense를 더 깊게 / 얕게 쌓던가 Dense의 뉴런 갯수(1024,512,256,...)를 수정한다던가 해야함! 
    Dense(512, activation='relu'),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    # Classification을 위한 Softmax 
    Dense(10, activation='softmax'), # 건들지 말라고 문구가 써져있긴 함.
])

model.summary()

"""## 컴파일 (compile)

1. `optimizer`는 가장 최적화가 잘되는 알고리즘인 'adam'을 사용합니다.
2. `loss`설정
  * 출력층 activation이 `sigmoid` 인 경우: `binary_crossentropy`
  * 출력층 activation이 `softmax` 인 경우: 
    * 원핫인코딩(O): `categorical_crossentropy`
    * 원핫인코딩(X): `sparse_categorical_crossentropy`)
3. `metrics`를 'acc' 혹은 'accuracy'로 지정하면, 학습시 정확도를 모니터링 할 수 있습니다.

**원핫인코딩 여부 체크방법**

케이스 1: 원핫인코딩 (X) 
+ 판별: y_train[0] 하나 출력해본다.
"""

y_train[0]

"""케이스 2: 원핫인코딩 (O) """

print(tf.one_hot(y_train[0], 10))

"""model.compile()"""

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])

"""## ModelCheckpoint: 체크포인트 생성

`val_loss` 기준으로 epoch 마다 최적의 모델을 저장하기 위하여, ModelCheckpoint를 만듭니다.
* `checkpoint_path`는 모델이 저장될 파일 명을 설정합니다.
* `ModelCheckpoint`을 선언하고, 적절한 옵션 값을 지정합니다.
"""

checkpoint_path = "my_checkpoint.ckpt"
checkpoint = ModelCheckpoint(filepath=checkpoint_path, 
                             save_weights_only=True, 
                             save_best_only=True, 
                             monitor='val_loss', 
                             verbose=1)

"""## 학습 (fit)

1. `validation_data`를 반드시 지정합니다.
2. `epochs`을 적절하게 지정합니다.
3. `callbacks`에 바로 위에서 만든 checkpoint를 지정합니다.
"""

history = model.fit(x_train, y_train,
                    validation_data=(x_valid, y_valid),
                    epochs=20,
                    callbacks=[checkpoint],
                   )

from google.colab import drive
drive.mount('/content/drive')

"""## 학습 완료 후 Load Weights (ModelCheckpoint)

학습이 완료된 후에는 반드시 `load_weights`를 해주어야 합니다.

그렇지 않으면, 열심히 ModelCheckpoint를 만든 의미가 없습니다.
"""

# checkpoint 를 저장한 파일명을 입력합니다.
model.load_weights(checkpoint_path)

"""## 학습한 후 검증하고 싶다면?"""

model.evaluate(x_valid, y_valid)

"""## 학습 Loss (오차)에 대한 시각화"""

plt.figure(figsize=(12, 9))
plt.plot(np.arange(1, 21), history.history['loss'])
plt.plot(np.arange(1, 21), history.history['val_loss'])
plt.title('Loss / Val Loss', fontsize=20)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['loss', 'val_loss'], fontsize=15)
plt.show()

"""### loss가 낮고, acc가 높으면 좋은건 맞음. 그런데 acc가 높으면서 loss도 높은 경우도 있다. 그래서 왠만하면 loss기준으로 체크해주면 더 정확함 """

plt.figure(figsize=(12, 9))
plt.plot(np.arange(1, 21), history.history['acc'])
plt.plot(np.arange(1, 21), history.history['val_acc'])
plt.title('Acc / Val Acc', fontsize=20)
plt.xlabel('Epochs')
plt.ylabel('Acc')
plt.legend(['acc', 'val_acc'], fontsize=15)
plt.show()